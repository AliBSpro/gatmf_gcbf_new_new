import os
import sys
# ç¡®ä¿èƒ½æ‰¾åˆ°å½“å‰ç›®å½•çš„æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

# external packages
import numpy as np
import time
import torch
import copy
import json
from tqdm import tqdm
import torch.nn.functional as F

# self writing files
import grid_networks as networks
import grid_model


class Buffer:
    def __init__(self, size, N, S_dim, A_dim, device):
        self.size = size
        self.N = N  # æ™ºèƒ½ä½“æ•°é‡
        self.S_dim = S_dim
        self.A_dim = A_dim
        self.device = device

        self.s = np.zeros((size, S_dim), dtype=np.float32)    # state
        self.s1 = np.zeros((size, S_dim), dtype=np.float32)   # next state
        self.a = np.zeros((size, A_dim), dtype=np.float32)    # action (logits or one-hot)
        self.r = np.zeros((size, N), dtype=np.float32)        # ğŸ”§ ä¿®å¤ï¼šä¸ªä½“åŒ–å¥–åŠ± (size, N)
        self.end = np.ones((size, N), dtype=np.float32)       # ğŸ”§ ä¿®å¤ï¼šä¸ªä½“åŒ–end_mask (size, N)

        self.pointer = 0
        self.now_size = 0

        # move to torch tensors on device
        self.s = torch.from_numpy(self.s).to(self.device)
        self.s1 = torch.from_numpy(self.s1).to(self.device)
        self.a = torch.from_numpy(self.a).to(self.device)
        self.r = torch.from_numpy(self.r).to(self.device)
        self.end = torch.from_numpy(self.end).to(self.device)

    def add(self, s, a, r, s1, end):
        self.s[self.pointer] = s
        if isinstance(a, np.ndarray):
            a = torch.from_numpy(a).to(self.device)
        self.a[self.pointer] = a
        # rå’Œendç°åœ¨éƒ½æ˜¯(N,)å½¢çŠ¶ï¼Œæ”¯æŒä¸ªä½“åŒ–
        self.r[self.pointer] = r
        self.s1[self.pointer] = s1
        if isinstance(end, np.ndarray):
            end = torch.from_numpy(end).to(self.device)
        self.end[self.pointer] = end

        self.pointer = (self.pointer + 1) % self.size
        self.now_size = min(self.now_size + 1, self.size)

    def sample(self, batch_size):
        indices = np.random.choice(self.now_size, size=batch_size, replace=False)
        s_batch = self.s[indices]
        a_batch = self.a[indices]
        r_batch = self.r[indices]
        s1_batch = self.s1[indices]
        end_batch = self.end[indices]
        return s_batch, a_batch, r_batch, s1_batch, end_batch


class MARL:    # Multi-agent reinforcement learning
    def __init__(self,
                 # â€”â€” ç¯å¢ƒç»“æ„ â€”â€”
                 num_grid=3,
                 num_agents=2,
                 num_obstacles=1,
                 env_max_steps=50,
                 env_seed=123,
                 fully_connected_adj=False,  # æ”¹ä¸ºå±€éƒ¨è¿æ¥

                 # â€”â€” é‡‡æ ·/è®­ç»ƒè®¾ç½® â€”â€”
                 max_steps=80,
                 max_episode=1500,
                 update_batch=32,
                 batch_size=128,
                 buffer_capacity=150_000,
                 update_interval=4,
                 save_interval=100,
                 eval_interval=25,
                 eval_episodes=50,

                 # â€”â€” ä¼˜åŒ–/ç¨³å®šæ€§ â€”â€”
                 lr=5e-5,
                 lr_decay=True,
                 grad_clip=True,
                 max_grad_norm=2.0,
                 soft_replace_rate=0.01,
                 gamma=0.85,

                 # â€”â€” æ¢ç´¢ç­–ç•¥ â€”â€”
                 explore_noise=0.1,
                 explore_noise_decay=True,
                 explore_decay=0.998,
                 explore_noise_min=0.01,

                 # â€”â€” å¥–åŠ±/åˆ°è¾¾é˜ˆå€¼ â€”â€”
                 arrival_bonus=15.0,
                 arrival_tol=1.0,
                 ):
        """
        GAT-MF å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨
        """
        # ä¿å­˜é…ç½®
        self.num_grid = num_grid
        self.N = num_agents
        self.num_obstacles = num_obstacles
        self.env_max_steps = env_max_steps
        self.env_seed = env_seed
        self.fully_connected_adj = fully_connected_adj
        
        self.max_steps = max_steps
        self.max_episode = max_episode
        self.update_batch = update_batch
        self.batch_size = batch_size
        self.buffer_capacity = buffer_capacity
        self.update_interval = update_interval
        self.save_interval = save_interval
        self.eval_interval = eval_interval
        self.eval_episodes = eval_episodes

        self.lr = lr
        self.lr_decay = lr_decay
        self.grad_clip = grad_clip
        self.max_grad_norm = max_grad_norm
        self.soft_replace_rate = soft_replace_rate
        self.gamma = gamma

        self.explore_noise = explore_noise
        self.explore_noise_decay = explore_noise_decay
        self.explore_decay = explore_decay
        self.explore_noise_min = explore_noise_min

        self.arrival_bonus = arrival_bonus
        self.arrival_tol = arrival_tol

        # environment / device / seeds
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        torch.manual_seed(9)
        if self.device.type == 'cuda':
            torch.cuda.manual_seed(9)
        np.random.seed(9)

        # model name/version & save dir
        self.version = f'GAT-MF'
        self.save_dir = os.path.join(os.path.dirname(__file__), '..', '..', 'model')
        os.makedirs(self.save_dir, exist_ok=True)

        print(f'ä½¿ç”¨è®¾å¤‡: {self.device}')
        print(f'ç¯å¢ƒè®¾ç½®: {self.num_grid}x{self.num_grid}ç½‘æ ¼, {self.N}ä¸ªæ™ºèƒ½ä½“, {self.num_obstacles}ä¸ªéšœç¢ç‰©')

        # create environment(simulator)
        self.simulator = grid_model.Model(
            grid_size_default=self.num_grid,
            num_agents_default=self.N,
            num_obstacles_default=self.num_obstacles,
            max_steps_default=self.env_max_steps,
            seed_default=self.env_seed,
            fully_connected_adj=self.fully_connected_adj
        )

        # è§‚æµ‹ç»´åº¦
        state_dim = self.simulator.output_dim

        # networks (ä½¿ç”¨å›ºå®šçš„ç½‘ç»œæ¶æ„ï¼Œä¸éœ€è¦å‚æ•°)
        self.actor = networks.Actor().to(self.device)
        self.actor_target = copy.deepcopy(self.actor).to(self.device)
        self.action_dim = 5  # ä»ç½‘ç»œå®šä¹‰å¯çŸ¥è¾“å‡ºç»´åº¦æ˜¯5

        self.actor_attention = networks.Attention().to(self.device)
        self.actor_attention_target = copy.deepcopy(self.actor_attention).to(self.device)

        self.critic = networks.Critic().to(self.device)
        self.critic_target = copy.deepcopy(self.critic).to(self.device)

        # ğŸ”§ æ·»åŠ ç¼ºå¤±çš„critic_attention - GAT-MFç®—æ³•å¿…éœ€
        self.critic_attention = networks.Attention().to(self.device)
        self.critic_attention_target = copy.deepcopy(self.critic_attention).to(self.device)

        self.opt_actor = torch.optim.Adam(params=self.actor.parameters(), lr=self.lr)
        self.opt_critic = torch.optim.Adam(params=self.critic.parameters(), lr=self.lr)
        
        # ğŸ”§ æ·»åŠ æ³¨æ„åŠ›ç½‘ç»œçš„ä¼˜åŒ–å™¨
        self.opt_actor_attention = torch.optim.Adam(params=self.actor_attention.parameters(), lr=self.lr)
        self.opt_critic_attention = torch.optim.Adam(params=self.critic_attention.parameters(), lr=self.lr)

        if self.lr_decay:
            self.lr_sche_actor = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt_actor, T_max=self.update_batch, eta_min=0)
            self.lr_sche_critic = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt_critic, T_max=self.update_batch, eta_min=0)
            # ğŸ”§ æ·»åŠ æ³¨æ„åŠ›ç½‘ç»œçš„å­¦ä¹ ç‡è°ƒåº¦å™¨
            self.lr_sche_actor_attention = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt_actor_attention, T_max=self.update_batch, eta_min=0)
            self.lr_sche_critic_attention = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt_critic_attention, T_max=self.update_batch, eta_min=0)

        # generate Adj matrix of graph (å·²åœ¨Modelåˆå§‹åŒ–æ—¶è®¡ç®—)
        self.Gmat = torch.from_numpy(self.simulator.Gmat).to(self.device)
        
        # åŠ¨æ€é‚»æ¥æ”¯æŒ
        self.use_dynamic_adjacency = getattr(self.simulator, 'use_dynamic_adjacency', False)
        if self.use_dynamic_adjacency:
            print("ğŸ”— è®­ç»ƒå™¨å¯ç”¨åŠ¨æ€å±€éƒ¨è¿æ¥")
        else:
            print("â„¹ï¸  è®­ç»ƒå™¨ä½¿ç”¨é™æ€é‚»æ¥çŸ©é˜µ")

        # experience buffer (flat state/action per step)
        self.buffer = Buffer(size=self.buffer_capacity,
                             N=self.N,
                             S_dim=self.N * state_dim,
                             A_dim=self.N * self.action_dim,
                             device=self.device)

        print(f'ç½‘ç»œåˆå§‹åŒ–å®Œæˆ - çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {self.action_dim}')
        print('â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”')

    # ================= helpers =================
    def soft_replace(self, target, source):
        for tar, src in zip(target.parameters(), source.parameters()):
            tar.data.copy_((1.0 - self.soft_replace_rate) * tar.data + self.soft_replace_rate * src.data)

    def get_action(self, action):
        action_vector = F.softmax(action, dim=-1)
        return action_vector

    def get_entropy(self, action):
        weight = F.softmax(action, dim=1)
        action_entropy = -torch.sum(weight * torch.log2(weight))
        return action_entropy

    def get_current_adjacency(self):
        """è·å–å½“å‰æ—¶åˆ»çš„é‚»æ¥çŸ©é˜µï¼ˆå¼ é‡ï¼‰"""
        if self.use_dynamic_adjacency and hasattr(self.simulator, 'get_dynamic_adjacency'):
            # ä½¿ç”¨åŠ¨æ€é‚»æ¥
            G_tensor = self.simulator.get_dynamic_adjacency(as_tensor=True)
            return G_tensor.to(self.device)
        else:
            # ä½¿ç”¨é™æ€é‚»æ¥
            return self.Gmat
    
    def _reached_flags(self, graph):
        # ä½¿ç”¨æ›¼å“ˆé¡¿è·ç¦»ï¼šåˆ°è¾¾å½“ä¸”ä»…å½“ L1 è·ç¦»ä¸º 0ï¼ˆå®Œå…¨é‡åˆï¼‰
        agent_pos = graph.env_states.agent  # (N, 2) torch.tensor
        goal_pos = graph.env_states.goal    # (N, 2) torch.tensor
        manh = torch.sum(torch.abs(agent_pos - goal_pos), dim=1)
        return manh == 0  # (N,) boolean tensor
    
    def _safety_flags(self, graph):
        """è®¡ç®—æ¯ä¸ªæ™ºèƒ½ä½“çš„å®‰å…¨çŠ¶æ€ï¼šä¸ä¸å…¶ä»–æ™ºèƒ½ä½“æˆ–éšœç¢ç‰©ç¢°æ’ï¼ˆæ›¼å“ˆé¡¿è·ç¦»â‰¤1 è§†ä¸ºä¸å®‰å…¨ï¼‰"""
        agent_pos = graph.env_states.agent  # (N, 2) torch.tensor
        N = agent_pos.shape[0]
        
        # æ™ºèƒ½ä½“é—´ç¢°æ’æ£€æŸ¥ (æ›¼å“ˆé¡¿è·ç¦» â‰¤ 1)
        diffs = agent_pos[:, None, :] - agent_pos[None, :, :]  # (N, N, 2)
        manh = torch.sum(torch.abs(diffs), dim=-1)  # (N, N) L1æ›¼å“ˆé¡¿è·ç¦»
        unsafe_agent_agent = (manh <= 1) & (~torch.eye(N, dtype=torch.bool, device=self.device))
        unsafe_from_agents = torch.any(unsafe_agent_agent, dim=1)  # (N,)
        
        # æ™ºèƒ½ä½“ä¸éšœç¢ç‰©ç¢°æ’æ£€æŸ¥
        obs_pos = graph.env_states.obstacle.positions  # (num_obstacles, 2)
        if obs_pos.numel() > 0:
            diffs_obs = agent_pos[:, None, :] - obs_pos[None, :, :]  # (N, num_obstacles, 2)
            manh_obs = torch.sum(torch.abs(diffs_obs), dim=-1)  # (N, num_obstacles)
            unsafe_from_obstacles = torch.any(manh_obs <= 1, dim=1)  # (N,)
        else:
            unsafe_from_obstacles = torch.zeros(N, dtype=torch.bool, device=self.device)
        
        # å®‰å…¨ = ä¸ä¸å…¶ä»–æ™ºèƒ½ä½“ç¢°æ’ AND ä¸ä¸éšœç¢ç‰©ç¢°æ’
        safe = ~(unsafe_from_agents | unsafe_from_obstacles)
        return safe  # (N,) boolean tensor

    # ================= evaluate =================
    def evaluate_policy(self, save_history: bool = False):
        """
        è¯„ä¼°é˜¶æ®µï¼šä¸åŠ å™ªå£°ï¼›åˆ°è¾¾å³é”å®šï¼ˆstayï¼‰ï¼›ä¸è®­ç»ƒçš„ reached åˆ¤å®šä¸€è‡´ã€‚
        """
        total_return = 0.0
        total_arrival = 0.0
        total_safety = 0.0  # æ·»åŠ å®‰å…¨ç‡ç»Ÿè®¡
        stay_idx = 0  # è‹¥"åŸåœ°"ç´¢å¼•ä¸åŒï¼Œè¯·æ”¹

        for _ in range(self.eval_episodes):
            self.simulator.reset()
            current_state = self.simulator.output_record()
            agents_done = torch.zeros(self.N, dtype=torch.bool, device=self.device)
            reached_ever = torch.zeros(self.N, dtype=torch.bool, device=self.device)

            ep_return = 0.0
            ep_safety_steps = 0  # ç»Ÿè®¡æœ¬episodeçš„å®‰å…¨æ­¥æ•°
            total_steps = 0      # ç»Ÿè®¡æœ¬episodeçš„æ€»æ­¥æ•°

            for _t in range(self.max_steps):
                with torch.no_grad():
                    state = torch.FloatTensor(current_state).to(self.device).unsqueeze(0)  # (1, N, F)
                    
                    # ä½¿ç”¨åŠ¨æ€é‚»æ¥çŸ©é˜µ
                    current_adj = self.get_current_adjacency()
                    
                    attn = self.actor_attention(state, current_adj)
                    state_bar = torch.bmm(attn, state)
                    state_all = torch.concat([state, state_bar], dim=-1)
                    logits = self.actor(state_all)
                    probs = self.get_action(logits).squeeze(0)  # (N, A)

                # é”å®šå·²åˆ°è¾¾
                stay = torch.zeros_like(probs)
                stay[:, stay_idx] = 1.0
                probs = torch.where(agents_done[:, None], stay, probs)

                # ç¯å¢ƒæ¨è¿›
                action_vector = probs.detach().cpu().numpy()
                reward_old = self.simulator.get_reward()
                self.simulator.move_miner(action_vector)
                reward_new = self.simulator.get_reward()
                r = float((reward_new - reward_old).mean())
                ep_return += r

                # æ›´æ–°åˆ°è¾¾
                graph = self.simulator.graph
                reached_now = self._reached_flags(graph).to(self.device)
                reached_ever = reached_ever | reached_now
                agents_done = agents_done | reached_now
                
                # è®¡ç®—å®‰å…¨ç‡
                safety_now = self._safety_flags(graph).to(self.device)  # (N,) å½“å‰æ­¥å®‰å…¨çŠ¶æ€
                ep_safety_steps += float(safety_now.float().mean().item())  # ç´¯ç§¯å®‰å…¨æ­¥æ•°
                total_steps += 1

                current_state = self.simulator.output_record()
                if bool(agents_done.all().item()):
                    break

            total_return += ep_return
            total_arrival += float(reached_ever.float().mean().item())
            # è®¡ç®—æœ¬episodeçš„å¹³å‡å®‰å…¨ç‡
            ep_safety_rate = ep_safety_steps / max(1, total_steps)
            total_safety += ep_safety_rate

        avg_return = total_return / max(1, self.eval_episodes)
        avg_arrival = total_arrival / max(1, self.eval_episodes)
        avg_safety = total_safety / max(1, self.eval_episodes)
        print(f"[Eval] return={avg_return:.4f}, arrival={avg_arrival:.4f}, safety={avg_safety:.4f}")
        return avg_return, avg_arrival, avg_safety

    # ================= update =================
    def update_network(self):
        """
        ä» buffer é‡‡æ ·æ›´æ–°ï¼š
          - target critic/actor è®¡ç®— bootstrap ç›®æ ‡ yï¼›
          - å…ˆæ›´æ–° criticï¼Œå†æ›´æ–° actorï¼›
          - è½¯æ›´æ–° targetã€‚
        """
        if self.buffer.now_size < self.batch_size:
            return

        # æ‰§è¡Œå¤šæ¬¡æ›´æ–°
        for update_step in range(self.update_batch):
            # æ¯æ¬¡æ›´æ–°éƒ½é‡æ–°é‡‡æ ·
            s_batch, a_batch, r_batch, s1_batch, end_batch = self.buffer.sample(self.batch_size)

            # reshape back to (B, N, *) for networks
            B = s_batch.shape[0]
            state_dim_each = int(s_batch.shape[1] // self.N)
            s_batch = s_batch.view(B, self.N, state_dim_each)
            s1_batch = s1_batch.view(B, self.N, state_dim_each)
            a_batch = a_batch.view(B, self.N, self.action_dim)  # logits or one-hot

            # ğŸ”§ ä¿®å¤ï¼šä¸ªä½“åŒ–Criticç›®æ¨™è®¡ç®—ï¼Œä½¿ç”¨çœŸæ­£çš„GAT-MFç®—æ³•
            with torch.no_grad():
                # ä½¿ç”¨å½“å‰åŠ¨æ€é‚»æ¥çŸ©é˜µ
                current_adj = self.get_current_adjacency()
                
                # Actoréƒ¨åˆ†ï¼šä½¿ç”¨actor_attention
                attn_actor_t = self.actor_attention_target(s1_batch, current_adj)
                s1_bar_actor = torch.bmm(attn_actor_t, s1_batch)
                s1_all_actor = torch.concat([s1_batch, s1_bar_actor], dim=-1)
                a1_logits = self.actor_target(s1_all_actor)
                a1 = self.get_action(a1_logits)                 # (B, N, A)
                
                # Criticéƒ¨åˆ†ï¼šä½¿ç”¨critic_attention
                attn_critic_t = self.critic_attention_target(s1_batch, current_adj)
                s1_bar_critic = torch.bmm(attn_critic_t, s1_batch)
                s1_all_critic = torch.concat([s1_batch, s1_bar_critic], dim=-1)
                a1_bar = torch.bmm(attn_critic_t, a1)           # å¯¹åŠ¨ä½œä¹Ÿåº”ç”¨æ³¨æ„åŠ›
                a1_all = torch.concat([a1, a1_bar], dim=-1)     # æ‹¼æ¥åŸå§‹åŠ¨ä½œå’Œæ³¨æ„åŠ›å¢å¼ºåŠ¨ä½œ
                q1 = self.critic_target(s1_all_critic, a1_all) # (B, N, 1)
                
                # ä¸ªä½“åŒ–ç›®æ¨™ï¼šr_batchå’Œend_batchç°åœ¨æ˜¯(B, N)
                r_batch_expanded = r_batch.unsqueeze(-1)        # (B, N, 1)
                end_batch_expanded = end_batch.unsqueeze(-1)    # (B, N, 1)
                y_individual = r_batch_expanded + self.gamma * q1 * end_batch_expanded  # (B, N, 1)
                y = y_individual.mean(dim=1)                    # (B, 1) ä»ç„¶å–å¹³å‡ç”¨æ–¼loss

            # Critic update - ä½¿ç”¨çœŸæ­£çš„GAT-MFç®—æ³•
            self.opt_critic.zero_grad()
            self.opt_critic_attention.zero_grad()
            
            # Criticä½¿ç”¨è‡ªå·±çš„attention
            attn_critic = self.critic_attention(s_batch, current_adj)
            s_bar_critic = torch.bmm(attn_critic, s_batch)
            s_all_critic = torch.concat([s_batch, s_bar_critic], dim=-1)
            
            # å¯¹åŠ¨ä½œä¹Ÿåº”ç”¨critic attention
            a_probs = F.softmax(a_batch, dim=-1)  # è‹¥ a_batch æ˜¯ logitsï¼Œåˆ™ softmax è½¬æ¦‚ç‡
            a_bar_critic = torch.bmm(attn_critic, a_probs)
            a_all_critic = torch.concat([a_probs, a_bar_critic], dim=-1)
            
            q = self.critic(s_all_critic, a_all_critic)          # (B, N, 1)
            q = q.mean(dim=1)                                    # (B, 1) å¯¹æ‰€æœ‰æ™ºèƒ½ä½“å–å¹³å‡
            critic_loss = F.mse_loss(q, y)
            critic_loss.backward()
            
            if self.grad_clip:
                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)
                torch.nn.utils.clip_grad_norm_(self.critic_attention.parameters(), self.max_grad_norm)
            
            self.opt_critic.step()
            self.opt_critic_attention.step()

            # Actor update - ä½¿ç”¨çœŸæ­£çš„GAT-MFç®—æ³•
            self.opt_actor.zero_grad()
            self.opt_actor_attention.zero_grad()
            
            # Actorä½¿ç”¨è‡ªå·±çš„attentionç”ŸæˆåŠ¨ä½œ
            attn_actor = self.actor_attention(s_batch, current_adj)
            s_bar_actor = torch.bmm(attn_actor, s_batch)
            s_all_actor = torch.concat([s_batch, s_bar_actor], dim=-1)
            a_logits = self.actor(s_all_actor)
            a = self.get_action(a_logits)
            
            # ä½†æ˜¯Qå€¼è¯„ä¼°éœ€è¦ä½¿ç”¨criticçš„attention (é‡æ–°è®¡ç®—ä»¥é¿å…è®¡ç®—å›¾å†²çª)
            with torch.no_grad():
                attn_critic_new = self.critic_attention(s_batch, current_adj)
                s_bar_critic_new = torch.bmm(attn_critic_new, s_batch)
                s_all_critic_new = torch.concat([s_batch, s_bar_critic_new], dim=-1)
            
            # å¯¹actorç”Ÿæˆçš„åŠ¨ä½œåº”ç”¨critic attention
            a_bar_critic_new = torch.bmm(attn_critic_new, a)
            a_all_critic_new = torch.concat([a, a_bar_critic_new], dim=-1)
            
            q_actor = self.critic(s_all_critic_new, a_all_critic_new)  # (B, N, 1)
            q_actor = q_actor.mean(dim=1)                              # (B, 1) å¯¹æ‰€æœ‰æ™ºèƒ½ä½“å–å¹³å‡
            actor_loss = - q_actor.mean()                              # æ ‡é‡æŸå¤±
            actor_loss.backward()
            
            if self.grad_clip:
                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm)
                torch.nn.utils.clip_grad_norm_(self.actor_attention.parameters(), self.max_grad_norm)
            
            self.opt_actor.step()
            self.opt_actor_attention.step()

        # æ›´æ–°å®Œæˆåè¿›è¡Œè½¯æ›´æ–°å’Œå­¦ä¹ ç‡è°ƒåº¦
        if self.lr_decay:
            self.lr_sche_actor.step()
            self.lr_sche_critic.step()
            self.lr_sche_actor_attention.step()
            self.lr_sche_critic_attention.step()

        # soft update - åŒ…å«æ‰€æœ‰ç½‘ç»œ
        self.soft_replace(self.actor_target, self.actor)
        self.soft_replace(self.actor_attention_target, self.actor_attention)
        self.soft_replace(self.critic_target, self.critic)
        self.soft_replace(self.critic_attention_target, self.critic_attention)

    # ================= train =================
    def train(self):
        """
        å…³é”®ï¼šåˆ°è¾¾å³ç»ˆæ­¢ï¼ˆæŒä¹…ï¼‰ï¼Œåˆ°è¾¾åªå¥–ä¸€æ¬¡ï¼Œå·²åˆ°è¾¾æŒç»­ end=0ï¼›ä¸æ”¹å˜ä½ åŸæœ‰ç½‘ç»œä¸ç¼“å†²ç»“æ„ã€‚
        """
        stay_idx = 0
        best_eval = -1e9
        
        # ç”¨äºè®°å½•è®­ç»ƒæŒ‡æ ‡
        episode_returns = []
        arrival_rates = []

        pbar = tqdm(total=self.max_episode, ncols=120)
        for episode in range(self.max_episode):
            # é‡ç½®ç¯å¢ƒ
            self.simulator.reset()
            
            # æŒ‡æ•°è¡°å‡æ¢ç´¢å™ªå£°ï¼ˆç¨³å®šï¼‰
            if self.explore_noise_decay:
                self.explore_noise = max(self.explore_noise_min, self.explore_noise * self.explore_decay)

            # åˆå§‹çŠ¶æ€
            current_state = self.simulator.output_record()  # (N, F)
            agents_done = torch.zeros(self.N, dtype=torch.bool, device=self.device)
            episode_return = 0.0

            for step in range(self.max_steps):
                # è®°å½• s (flatten to (N*F,))
                state_flat = torch.FloatTensor(current_state.flatten()).to(self.device)
                self.buffer.s[self.buffer.pointer] = state_flat

                with torch.no_grad():
                    state = torch.FloatTensor(current_state).to(self.device).unsqueeze(0)

                    # ä½¿ç”¨åŠ¨æ€é‚»æ¥çŸ©é˜µ
                    current_adj = self.get_current_adjacency()
                    
                    Actor_attention = self.actor_attention(state, current_adj)
                    Actor_state_bar = torch.bmm(Actor_attention, state)
                    Actor_state_all = torch.concat([state, Actor_state_bar], dim=-1)
                    action_logits = self.actor(Actor_state_all)   # (1, N, A)

                    # ğŸ”§ ä¿®å¤ï¼šæ¢ç´¢å™ªå£°è§£è€¦ï¼Œä½¿ç”¨å›ºå®šå¹…åº¦é¿å…åˆæœŸæ¢ç´¢ä¸è¶³
                    if self.explore_noise > 0:
                        noise_scale = 1.0  # å›ºå®šå™ªå£°å¹…åº¦ï¼Œä¸èˆ‡logitsè€¦åˆ
                        action_logits = action_logits + torch.randn_like(action_logits) * noise_scale * self.explore_noise

                    probs = self.get_action(action_logits).squeeze(0)  # (N, A)

                # åˆ°è¾¾è€… -> å¼ºåˆ¶ stay
                stay = torch.zeros_like(probs)
                stay[:, stay_idx] = 1.0
                probs = torch.where(agents_done[:, None], stay, probs)

                # ç¯å¢ƒå‰è¿›
                action_vector = probs.detach().cpu().numpy()
                reward_old = self.simulator.get_reward()
                self.simulator.move_miner(action_vector)
                reward_new = self.simulator.get_reward()
                reward = reward_new - reward_old                         # numpy (N,1)
                episode_return += float(reward.mean())

                # åˆ°è¾¾åˆ¤å®š & ä¸€æ¬¡æ€§å¥–åŠ±
                current_graph = self.simulator.graph
                reached_now = self._reached_flags(current_graph).to(self.device)   # (N,)
                first_hit = reached_now & (~agents_done)
                
                # ğŸ”§ ä¿®å¤ï¼šä¸ªä½“åŒ–å¥–åŠ±è®¡ç®—
                individual_rewards = reward.flatten()  # (N,) å€‹é«”å¥–åŠ±
                bonus_individual = torch.zeros(self.N, dtype=torch.float32, device=self.device)
                if first_hit.any():
                    bonus_individual[first_hit] = self.arrival_bonus
                    episode_return += float(bonus_individual.sum().item())
                
                total_individual_rewards = individual_rewards + bonus_individual.cpu().numpy()  # (N,)
                episode_return += float(total_individual_rewards.mean() - individual_rewards.mean())  # åªåŠ bonuséƒ¨åˆ†

                # æŒä¹…åˆ°è¾¾æ©ç æ›´æ–°
                agents_done = agents_done | reached_now

                # ğŸ”§ ä¿®å¤ï¼šä¸ªä½“åŒ–bufferå­˜å„²
                current_state = self.simulator.output_record()
                state_flat_next = torch.FloatTensor(current_state.flatten()).to(self.device)
                self.buffer.s1[self.buffer.pointer] = state_flat_next
                self.buffer.a[self.buffer.pointer] = torch.FloatTensor(action_logits.detach().cpu().numpy().flatten()).to(self.device)
                
                # å­˜å„²ä¸ªä½“åŒ–å¥–åŠ± (N,)
                self.buffer.r[self.buffer.pointer] = torch.FloatTensor(total_individual_rewards).to(self.device)

                # ğŸ”§ ä¿®å¤ï¼šç²¾ç´°åŒ–end_mask - æ™ºèƒ½ä½“ç´šåˆ¥æ§åˆ¶
                end_mask = torch.ones(self.N, dtype=torch.float32, device=self.device)
                end_mask[agents_done] = 0.0  # åªå°å·²åˆ°è¾¾çš„æ™ºèƒ½ä½“åœæ­¢bootstrap
                if step == self.max_steps - 1:
                    end_mask.fill_(0.0)  # æœ€å¾Œä¸€æ­¥å…¨éƒ¨åœæ­¢
                self.buffer.end[self.buffer.pointer] = end_mask

                # æŒ‡é’ˆ/å¤§å°
                self.buffer.now_size = max(self.buffer.now_size, self.buffer.pointer + 1)
                self.buffer.pointer = (self.buffer.pointer + 1) % self.buffer.size

                # æ‰€æœ‰ agent éƒ½å·²åˆ°è¾¾ â†’ æå‰ç»“æŸæœ¬ episode
                if bool(agents_done.all().item()):
                    break

            # è®°å½•å½“å‰episodeçš„åˆ°è¾¾ç‡
            current_arrival = float(agents_done.float().mean().item())
            episode_returns.append(episode_return)
            arrival_rates.append(current_arrival)

            # æ›´æ–°ç½‘ç»œ
            if self.buffer.now_size >= self.batch_size and (episode + 1) % self.update_interval == 0:
                try:
                    self.update_network()
                except Exception as e:
                    print(f"\nâŒ ç½‘ç»œæ›´æ–°å¤±è´¥ (Episode {episode+1}): {e}")
                    print(f"   - Bufferå¤§å°: {self.buffer.now_size}/{self.buffer.size}")
                    print(f"   - Batchå¤§å°: {self.batch_size}")
                    raise e

            # è¯„ä¼°/ä¿å­˜
            if self.eval_interval and (episode + 1) % self.eval_interval == 0:
                eval_ret, eval_arrival, eval_safety = self.evaluate_policy()
                if eval_ret > best_eval:
                    best_eval = eval_ret
                    if self.save_interval:
                        self.save_model(episode + 1, eval_ret, eval_arrival)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if self.save_interval and (episode + 1) % self.save_interval == 0:
                self.save_model(episode + 1, episode_return, current_arrival)

            # æ›´æ–°è¿›åº¦æ¡
            recent_arrival = np.mean(arrival_rates[-10:]) if len(arrival_rates) >= 10 else np.mean(arrival_rates)
            recent_return = np.mean(episode_returns[-10:]) if len(episode_returns) >= 10 else np.mean(episode_returns)
            pbar.set_postfix({
                'Arr': f'{recent_arrival:.3f}',
                'Ret': f'{recent_return:.2f}',
                'Noise': f'{self.explore_noise:.4f}',
                'Buf': f'{self.buffer.now_size}/{self.buffer.size}'
            })
            pbar.update(1)

        pbar.close()
        final_arrival = np.mean(arrival_rates[-100:]) if len(arrival_rates) >= 100 else np.mean(arrival_rates)
        print(f"\nè®­ç»ƒå®Œæˆï¼æœ€ç»ˆåˆ°è¾¾ç‡: {final_arrival:.3f}")
        return episode_returns, arrival_rates

    def save_model(self, episode, return_val, arrival_rate):
        """ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒæŒ‡æ ‡"""
        timestamp = time.strftime('%Y-%m-%d_%H-%M-%S', time.localtime())
        save_path = os.path.join(self.save_dir, f'{self.num_grid}grid_{timestamp}')
        os.makedirs(save_path, exist_ok=True)
        
        # ä¿å­˜æ¨¡å‹æƒé‡ - åŒ…å«æ‰€æœ‰ç½‘ç»œ
        torch.save(self.actor.state_dict(), os.path.join(save_path, f'actor_{episode}.pth'))
        torch.save(self.actor_target.state_dict(), os.path.join(save_path, f'actor_target_{episode}.pth'))
        torch.save(self.actor_attention.state_dict(), os.path.join(save_path, f'actor_attention_{episode}.pth'))
        torch.save(self.actor_attention_target.state_dict(), os.path.join(save_path, f'actor_attention_target_{episode}.pth'))
        torch.save(self.critic.state_dict(), os.path.join(save_path, f'critic_{episode}.pth'))
        torch.save(self.critic_target.state_dict(), os.path.join(save_path, f'critic_target_{episode}.pth'))
        # ğŸ”§ æ·»åŠ critic_attentionçš„ä¿å­˜
        torch.save(self.critic_attention.state_dict(), os.path.join(save_path, f'critic_attention_{episode}.pth'))
        torch.save(self.critic_attention_target.state_dict(), os.path.join(save_path, f'critic_attention_target_{episode}.pth'))
        
        # ä¿å­˜é…ç½®
        config = {
            'episode': episode,
            'return': float(return_val),
            'arrival_rate': float(arrival_rate),
            'num_grid': self.num_grid,
            'num_agents': self.N,
            'num_obstacles': self.num_obstacles,
            'max_steps': self.max_steps,
            'timestamp': timestamp
        }
        with open(os.path.join(save_path, 'config.json'), 'w') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        print(f"[Save] Episode {episode}: Return={return_val:.4f}, Arrival={arrival_rate:.4f} -> {save_path}")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    print("=" * 60)
    print("GAT-MF å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ è®­ç»ƒ")
    print("=" * 60)
    
    trainer = MARL(
        # â€”â€” ç¯å¢ƒå‚æ•° â€”â€”
        num_grid=3,
        num_agents=2,
        num_obstacles=1,
        env_max_steps=50,
        env_seed=123,
        fully_connected_adj=False,  # æ”¹ç‚ºå±€éƒ¨è¿æ¥

        # â€”â€” è®­ç»ƒå™¨é‡‡æ ·/ä¼˜åŒ– â€”â€”
        max_steps=80,                    # å¢åŠ æ­¥æ•°ï¼Œç»™æ›´å¤šæ¢ç´¢æ—¶é—´
        max_episode=2500,                # å±€éƒ¨è¿æ¥éœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°
        update_batch=16,                 # é™ä½æ›´æ–°æ‰¹æ¬¡ï¼Œé¿å…è¿‡åº¦æ›´æ–°
        batch_size=64,                   # é™ä½æ‰¹å¤§å°ï¼Œç¡®ä¿èƒ½å¤Ÿæ›´æ–°
        buffer_capacity=200_000,         # å¢åŠ ç¼“å­˜å®¹é‡
        update_interval=1,               # æ¯ä¸ªepisodeéƒ½å°è¯•æ›´æ–°
        save_interval=100,
        eval_interval=25,
        eval_episodes=50,                # å¢åŠ è¯„ä¼°episodeæ•°

        # â€”â€” ä¼˜åŒ–/ç¨³å®šæ€§ â€”â€”
        lr=8e-5,                         # å±€éƒ¨è¿æ¥éœ€è¦ç¨é«˜å­¦ä¹ ç‡
        lr_decay=True,
        grad_clip=True,
        max_grad_norm=2.0,               # é™ä½æ¢¯åº¦è£å‰ªé˜ˆå€¼
        soft_replace_rate=0.008,         # ç¨å¿«çš„targetæ›´æ–°é€‚åº”åŠ¨æ€é‚»æ¥
        gamma=0.88,                      # ç¨å¾®æé«˜gammaå¹³è¡¡çŸ­æœŸé•¿æœŸå¥–åŠ±

        # â€”â€” æ¢ç´¢ç­–ç•¥ â€”â€”
        explore_noise=0.15,              # å±€éƒ¨è¿æ¥éœ€è¦æ›´å¤šæ¢ç´¢
        explore_noise_decay=True,
        explore_decay=0.995,             # æ›´æ…¢çš„å™ªå£°è¡°å‡
        explore_noise_min=0.02,          # ä¿æŒæœ€å°æ¢ç´¢å™ªå£°

        # â€”â€” å¥–åŠ±/åˆ°è¾¾é˜ˆå€¼ â€”â€”
        arrival_bonus=15.0,              # å¢åŠ åˆ°è¾¾å¥–åŠ±
        arrival_tol=1.0,               # ç¦»æ•£ç½‘æ ¼ç²¾ç¡®åˆ°è¾¾åˆ¤å®šï¼Œåªæœ‰å®Œå…¨é‡åˆæ‰ç®—åˆ°è¾¾
    )
    
    try:
        episode_returns, arrival_rates = trainer.train()
        print(f"è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        if len(arrival_rates) > 0:
            print(f"å¹³å‡åˆ°è¾¾ç‡: {np.mean(arrival_rates):.3f}")
            if len(arrival_rates) >= 100:
                print(f"æœ€ç»ˆåˆ°è¾¾ç‡: {np.mean(arrival_rates[-100:]):.3f}")
            else:
                print(f"æœ€ç»ˆåˆ°è¾¾ç‡: {np.mean(arrival_rates[-len(arrival_rates):]):.3f}")
        else:
            print("è­¦å‘Šï¼šæ²¡æœ‰æ”¶é›†åˆ°åˆ°è¾¾ç‡æ•°æ®")
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        print("\nè°ƒè¯•ä¿¡æ¯ï¼š")
        print(f"- æ£€æŸ¥batch_size({trainer.batch_size})æ˜¯å¦è¿‡å¤§")
        print(f"- æ£€æŸ¥bufferå¤§å°({trainer.buffer.now_size})æ˜¯å¦è¶³å¤Ÿ")
        print(f"- æ£€æŸ¥update_interval({trainer.update_interval})è®¾ç½®")


if __name__ == '__main__':
    main()
